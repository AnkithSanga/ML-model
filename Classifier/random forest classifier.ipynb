from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd

 

df = pd.read_csv(r"heart.csv")

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

X = df.drop(['output'], axis=1)
Y = df['output']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

 

# Define the parameter grid for random search
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [5, 10, 15],
    'min_samples_split': [5, 10, 15],
    'min_samples_leaf': [2, 4, 6]
}

 

# Create the Random Forest Classifier
rfc = RandomForestClassifier(random_state=42)

 

# Perform random search with cross-validation
random_search = RandomizedSearchCV(rfc, param_grid, cv=5, n_iter=10, n_jobs=-1)
random_search.fit(X_train, y_train)

 

# Get the best hyperparameters
best_params = random_search.best_params_

 

# Train the classifier with the best hyperparameters
classifier = RandomForestClassifier(random_state=42, **best_params)
classifier.fit(X_train, y_train)

 

# Predict on the testing set
y_pred = classifier.predict(X_test)

 

# Calculate accuracy
accuracy = metrics.accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Predict on the training set
y_train_pred = classifier.predict(X_train)

 

# Predict on the testing set
y_test_pred = classifier.predict(X_test)

 

# Calculate training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
print("Training Accuracy:", train_accuracy)

 

# Calculate testing accuracy
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Testing Accuracy:", test_accuracy)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
# Generate confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
print("Confusion Matrix:")
print(cm)

 

precision = precision_score(y_test, y_test_pred)
print("Precision:", precision)

 

recall = recall_score(y_test, y_test_pred)
print("Recall:", recall)

 

f1 = f1_score(y_test, y_test_pred)
print("F1 Score:", f1)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_test_pred))

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

 

# Get the predicted probabilities for the positive class
y_test_probs = classifier.predict_proba(X_test)[:, 1]

 

# Calculate the false positive rate, true positive rate, and thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_test_probs)

 

# Calculate the AUC score
auc_score = auc(fpr, tpr)

 

# Plot the ROC curve
plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(auc_score))
plt.plot([0, 1], [0, 1], 'k--')  # Random guess curve (diagonal)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

 

# Print the AUC score
print("AUC Score:", auc_score)

